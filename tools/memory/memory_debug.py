"""
Memory Debug Tools for Tiny Torch - æ˜¾å­˜è°ƒè¯•å·¥å…·é›†
æ•´åˆäº†åˆ†å¸ƒå¼è®­ç»ƒåœºæ™¯çš„æ˜¾å­˜è°ƒè¯•åŠŸèƒ½

ä¸»è¦åŠŸèƒ½:
1. æ˜¾å­˜ä½¿ç”¨ç›‘æ§å’Œåˆ†æ (Memory Profiler)
2. OOMæ£€æµ‹å’Œé¢„è­¦ (OOM Detector) 
3. æ˜¾å­˜ç¢ç‰‡åˆ†æ (Fragmentation Analyzer)
4. å†…å­˜æ³„æ¼æ£€æµ‹ (Memory Leak Detector)
5. åˆ†å¸ƒå¼å†…å­˜ç›‘æ§ (Distributed Memory Monitor)
6. ç»Ÿä¸€çš„å‘½ä»¤è¡Œç•Œé¢ (CLI)

ä½¿ç”¨ç¤ºä¾‹:
    from tools.memory import MemoryDebugger
    
    # åŸºæœ¬ç›‘æ§
    debugger = MemoryDebugger()
    debugger.start_monitoring()
    
    # å‘½ä»¤è¡Œä½¿ç”¨
    python -m tools.memory profile --duration 60
    python -m tools.memory oom --threshold 85
"""

import os
import gc
import time
import json
import threading
import socket
import pickle
import signal
import subprocess
import argparse
import sys
import traceback
import weakref
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import queue
import multiprocessing as mp

# ç¬¬ä¸‰æ–¹ä¾èµ–
try:
    import numpy as np
    import matplotlib.pyplot as plt
    import psutil
except ImportError as e:
    print(f"è­¦å‘Š: ç¼ºå°‘ä¾èµ–é¡¹ {e}. è¯·è¿è¡Œ: pip install numpy matplotlib psutil")
    sys.exit(1)


# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

class OOMRiskLevel(Enum):
    """OOMé£é™©ç­‰çº§"""
    LOW = "low"
    MEDIUM = "medium" 
    HIGH = "high"
    CRITICAL = "critical"


class FragmentationLevel(Enum):
    """ç¢ç‰‡åŒ–ç¨‹åº¦ç­‰çº§"""
    LOW = "low"
    MODERATE = "moderate"
    HIGH = "high"
    SEVERE = "severe"


class DistributedRole(Enum):
    """åˆ†å¸ƒå¼è§’è‰²"""
    MASTER = "master"
    WORKER = "worker"
    STANDALONE = "standalone"


@dataclass
class MemorySnapshot:
    """æ˜¾å­˜å¿«ç…§æ•°æ®ç»“æ„"""
    timestamp: float
    device_id: int
    allocated: int  # å·²åˆ†é…æ˜¾å­˜ (bytes)
    reserved: int   # é¢„ç•™æ˜¾å­˜ (bytes) 
    free: int       # å¯ç”¨æ˜¾å­˜ (bytes)
    total: int      # æ€»æ˜¾å­˜ (bytes)
    utilization: float  # åˆ©ç”¨ç‡ (%)
    fragmentation: float  # ç¢ç‰‡ç‡ (%)
    process_info: Dict[str, Any] = field(default_factory=dict)


@dataclass
class OOMEvent:
    """OOMäº‹ä»¶è®°å½•"""
    timestamp: float
    device_id: int
    requested_memory: int
    available_memory: int
    total_memory: int
    stack_trace: Optional[str] = None
    context_info: Optional[Dict[str, Any]] = None


@dataclass
class OOMPrediction:
    """OOMé¢„æµ‹ç»“æœ"""
    device_id: int
    risk_level: OOMRiskLevel
    predicted_oom_time: Optional[float]
    confidence: float  # é¢„æµ‹ç½®ä¿¡åº¦ (0-1)
    current_usage: float
    trend_slope: float
    recommended_actions: List[str] = field(default_factory=list)


@dataclass
class MemoryLeakEvent:
    """å†…å­˜æ³„æ¼äº‹ä»¶"""
    timestamp: float
    device_id: int
    leaked_memory: int
    leak_rate: float
    potential_sources: List[str] = field(default_factory=list)
    stack_trace: Optional[str] = None
    severity: str = "medium"


# ============================================================================
# æ ¸å¿ƒå·¥å…·ç±»
# ============================================================================

class MemoryProfiler:
    """æ˜¾å­˜ä½¿ç”¨åˆ†æå™¨"""
    
    def __init__(self, 
                 devices: Optional[List[int]] = None,
                 sampling_interval: float = 0.1,
                 max_snapshots: int = 10000):
        self.devices = devices or self._get_available_devices()
        self.sampling_interval = sampling_interval
        self.max_snapshots = max_snapshots
        self.snapshots: Dict[int, List[MemorySnapshot]] = defaultdict(list)
        self.monitoring = False
        self.monitor_thread = None
        
    def _get_available_devices(self) -> List[int]:
        """è·å–å¯ç”¨çš„GPUè®¾å¤‡"""
        try:
            result = subprocess.run(['nvidia-smi', '-L'], 
                                 capture_output=True, text=True, check=True)
            devices = []
            for line in result.stdout.strip().split('\n'):
                if 'GPU' in line:
                    device_id = int(line.split(':')[0].split()[-1])
                    devices.append(device_id)
            return devices
        except (subprocess.CalledProcessError, FileNotFoundError):
            return []
    
    def _get_memory_info(self, device_id: int) -> Optional[MemorySnapshot]:
        """è·å–æŒ‡å®šè®¾å¤‡çš„å†…å­˜ä¿¡æ¯"""
        try:
            cmd = f"nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv,noheader,nounits -i {device_id}"
            result = subprocess.run(cmd.split(), capture_output=True, text=True, check=True)
            used, free, total = map(int, result.stdout.strip().split(', '))
            
            # è½¬æ¢ä¸ºå­—èŠ‚
            used_bytes = used * 1024 * 1024
            free_bytes = free * 1024 * 1024 
            total_bytes = total * 1024 * 1024
            reserved_bytes = total_bytes - free_bytes
            
            utilization = (used_bytes / total_bytes) * 100 if total_bytes > 0 else 0
            fragmentation = self._calculate_fragmentation(device_id, free_bytes, total_bytes)
            
            return MemorySnapshot(
                timestamp=time.time(),
                device_id=device_id,
                allocated=used_bytes,
                reserved=reserved_bytes,
                free=free_bytes,
                total=total_bytes,
                utilization=utilization,
                fragmentation=fragmentation,
                process_info=self._get_process_info()
            )
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è·å–è®¾å¤‡ {device_id} çš„å†…å­˜ä¿¡æ¯: {e}")
            return None
    
    def _calculate_fragmentation(self, device_id: int, free_bytes: int, total_bytes: int) -> float:
        """è®¡ç®—å†…å­˜ç¢ç‰‡ç‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        if total_bytes == 0:
            return 0.0
        return max(0, min(100, (1 - free_bytes / total_bytes) * 50))
    
    def _get_process_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä¿¡æ¯"""
        try:
            process = psutil.Process()
            return {
                'pid': process.pid,
                'memory_percent': process.memory_percent(),
                'memory_info': process.memory_info()._asdict(),
                'cpu_percent': process.cpu_percent()
            }
        except:
            return {}
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§ {len(self.devices)} ä¸ªGPUè®¾å¤‡")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        print("ğŸ”„ åœæ­¢å†…å­˜ç›‘æ§")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            for device_id in self.devices:
                snapshot = self._get_memory_info(device_id)
                if snapshot:
                    self.snapshots[device_id].append(snapshot)
                    # é™åˆ¶å¿«ç…§æ•°é‡
                    if len(self.snapshots[device_id]) > self.max_snapshots:
                        self.snapshots[device_id].pop(0)
            
            time.sleep(self.sampling_interval)
    
    def get_current_status(self) -> Dict[int, Dict[str, Any]]:
        """è·å–å½“å‰çŠ¶æ€"""
        status = {}
        for device_id in self.devices:
            if device_id in self.snapshots and self.snapshots[device_id]:
                latest = self.snapshots[device_id][-1]
                status[device_id] = {
                    'current_used': latest.allocated,
                    'total': latest.total,
                    'utilization': latest.utilization,
                    'fragmentation': latest.fragmentation,
                    'free': latest.free
                }
        return status
    
    def generate_report(self, output_file: str) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'devices': {},
            'summary': {}
        }
        
        total_memory = 0
        total_used = 0
        
        for device_id, snapshots in self.snapshots.items():
            if not snapshots:
                continue
                
            latest = snapshots[-1]
            utilizations = [s.utilization for s in snapshots]
            
            device_report = {
                'device_id': device_id,
                'current_usage': {
                    'allocated': latest.allocated,
                    'free': latest.free,
                    'total': latest.total,
                    'utilization_percent': latest.utilization
                },
                'statistics': {
                    'avg_utilization': np.mean(utilizations),
                    'max_utilization': np.max(utilizations),
                    'min_utilization': np.min(utilizations),
                    'samples_count': len(snapshots)
                },
                'fragmentation': latest.fragmentation
            }
            
            report['devices'][device_id] = device_report
            total_memory += latest.total
            total_used += latest.allocated
        
        report['summary'] = {
            'total_devices': len(self.devices),
            'total_memory_gb': total_memory / (1024**3),
            'total_used_gb': total_used / (1024**3),
            'overall_utilization': (total_used / total_memory * 100) if total_memory > 0 else 0
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report


class OOMDetector:
    """OOMæ£€æµ‹å’Œé¢„è­¦å™¨"""
    
    def __init__(self, 
                 threshold: float = 85.0,
                 prediction_window: int = 10,
                 warning_callback: Optional[Callable] = None):
        self.threshold = threshold
        self.prediction_window = prediction_window
        self.warning_callback = warning_callback or self._default_warning_callback
        self.monitoring = False
        self.monitor_thread = None
        self.memory_history: Dict[int, List[Tuple[float, float]]] = defaultdict(list)
        
    def _default_warning_callback(self, prediction: OOMPrediction):
        """é»˜è®¤è­¦å‘Šå›è°ƒ"""
        print(f"âš ï¸  OOMè­¦å‘Š: GPU {prediction.device_id}")
        print(f"   é£é™©ç­‰çº§: {prediction.risk_level.value}")
        print(f"   å½“å‰ä½¿ç”¨ç‡: {prediction.current_usage:.1f}%")
        if prediction.predicted_oom_time:
            print(f"   é¢„æµ‹OOMæ—¶é—´: {prediction.predicted_oom_time:.1f}ç§’å")
        print(f"   ç½®ä¿¡åº¦: {prediction.confidence:.2f}")
        for action in prediction.recommended_actions:
            print(f"   å»ºè®®: {action}")
    
    def start_monitoring(self, devices: Optional[List[int]] = None):
        """å¼€å§‹OOMç›‘æ§"""
        if self.monitoring:
            return
            
        self.devices = devices or self._get_available_devices()
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print(f"ğŸš¨ å¼€å§‹OOMç›‘æ§ (é˜ˆå€¼: {self.threshold}%)")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        print("ğŸ”„ åœæ­¢OOMç›‘æ§")
    
    def _get_available_devices(self) -> List[int]:
        """è·å–å¯ç”¨è®¾å¤‡"""
        try:
            result = subprocess.run(['nvidia-smi', '-L'], 
                                 capture_output=True, text=True, check=True)
            devices = []
            for line in result.stdout.strip().split('\n'):
                if 'GPU' in line:
                    device_id = int(line.split(':')[0].split()[-1])
                    devices.append(device_id)
            return devices
        except:
            return []
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            for device_id in self.devices:
                utilization = self._get_device_utilization(device_id)
                if utilization is not None:
                    self.memory_history[device_id].append((time.time(), utilization))
                    
                    # é™åˆ¶å†å²è®°å½•é•¿åº¦
                    if len(self.memory_history[device_id]) > 100:
                        self.memory_history[device_id].pop(0)
                    
                    # è¿›è¡ŒOOMé¢„æµ‹
                    prediction = self._predict_oom(device_id)
                    if prediction and prediction.risk_level in [OOMRiskLevel.HIGH, OOMRiskLevel.CRITICAL]:
                        self.warning_callback(prediction)
            
            time.sleep(1.0)
    
    def _get_device_utilization(self, device_id: int) -> Optional[float]:
        """è·å–è®¾å¤‡åˆ©ç”¨ç‡"""
        try:
            cmd = f"nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits -i {device_id}"
            result = subprocess.run(cmd.split(), capture_output=True, text=True, check=True)
            used, total = map(int, result.stdout.strip().split(', '))
            return (used / total) * 100 if total > 0 else 0
        except:
            return None
    
    def _predict_oom(self, device_id: int) -> Optional[OOMPrediction]:
        """é¢„æµ‹OOMé£é™©"""
        history = self.memory_history.get(device_id, [])
        if len(history) < 3:
            return None
        
        recent_usage = [usage for _, usage in history[-self.prediction_window:]]
        current_usage = recent_usage[-1]
        
        # è®¡ç®—è¶‹åŠ¿
        times = list(range(len(recent_usage)))
        if len(times) > 1:
            trend_slope = np.polyfit(times, recent_usage, 1)[0]
        else:
            trend_slope = 0
        
        # è¯„ä¼°é£é™©
        risk_level = OOMRiskLevel.LOW
        predicted_time = None
        confidence = 0.5
        actions = []
        
        if current_usage > 95:
            risk_level = OOMRiskLevel.CRITICAL
            confidence = 0.95
            actions = ["ç«‹å³é‡Šæ”¾ç¼“å­˜: tiny_torch.cuda.empty_cache()", "å‡å°‘batch size", "æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼"]
        elif current_usage > self.threshold:
            if trend_slope > 1.0:
                risk_level = OOMRiskLevel.HIGH
                predicted_time = (100 - current_usage) / trend_slope
                confidence = 0.8
                actions = ["å‡†å¤‡é‡Šæ”¾ç¼“å­˜", "è€ƒè™‘å‡å°‘batch size"]
            else:
                risk_level = OOMRiskLevel.MEDIUM
                confidence = 0.6
                actions = ["ç›‘æ§å†…å­˜ä½¿ç”¨è¶‹åŠ¿"]
        
        return OOMPrediction(
            device_id=device_id,
            risk_level=risk_level,
            predicted_oom_time=predicted_time,
            confidence=confidence,
            current_usage=current_usage,
            trend_slope=trend_slope,
            recommended_actions=actions
        )


class MemoryLeakDetector:
    """å†…å­˜æ³„æ¼æ£€æµ‹å™¨"""
    
    def __init__(self, leak_threshold: float = 100 * 1024 * 1024):  # 100MB
        self.leak_threshold = leak_threshold
        self.monitoring = False
        self.baseline_memory: Dict[int, int] = {}
        self.leak_events: List[MemoryLeakEvent] = []
        
    def start_monitoring(self, devices: Optional[List[int]] = None):
        """å¼€å§‹æ³„æ¼æ£€æµ‹"""
        self.devices = devices or self._get_available_devices()
        
        # å»ºç«‹åŸºçº¿
        for device_id in self.devices:
            usage = self._get_device_memory_usage(device_id)
            if usage is not None:
                self.baseline_memory[device_id] = usage
        
        self.monitoring = True
        print("ğŸ” å¼€å§‹å†…å­˜æ³„æ¼æ£€æµ‹")
    
    def stop_monitoring(self):
        """åœæ­¢æ£€æµ‹"""
        self.monitoring = False
        print("ğŸ”„ åœæ­¢å†…å­˜æ³„æ¼æ£€æµ‹")
    
    def _get_available_devices(self) -> List[int]:
        """è·å–å¯ç”¨è®¾å¤‡"""
        try:
            result = subprocess.run(['nvidia-smi', '-L'], 
                                 capture_output=True, text=True, check=True)
            devices = []
            for line in result.stdout.strip().split('\n'):
                if 'GPU' in line:
                    device_id = int(line.split(':')[0].split()[-1])
                    devices.append(device_id)
            return devices
        except:
            return []
    
    def _get_device_memory_usage(self, device_id: int) -> Optional[int]:
        """è·å–è®¾å¤‡å†…å­˜ä½¿ç”¨é‡"""
        try:
            cmd = f"nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i {device_id}"
            result = subprocess.run(cmd.split(), capture_output=True, text=True, check=True)
            return int(result.stdout.strip()) * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        except:
            return None
    
    def check_for_leaks(self) -> List[MemoryLeakEvent]:
        """æ£€æŸ¥å†…å­˜æ³„æ¼"""
        if not self.monitoring:
            return []
        
        current_leaks = []
        
        for device_id in self.devices:
            current_usage = self._get_device_memory_usage(device_id)
            baseline = self.baseline_memory.get(device_id)
            
            if current_usage is not None and baseline is not None:
                memory_increase = current_usage - baseline
                
                if memory_increase > self.leak_threshold:
                    leak_event = MemoryLeakEvent(
                        timestamp=time.time(),
                        device_id=device_id,
                        leaked_memory=memory_increase,
                        leak_rate=memory_increase / 3600,  # ç®€åŒ–è®¡ç®—
                        potential_sources=["æœªçŸ¥"],
                        severity="high" if memory_increase > self.leak_threshold * 2 else "medium"
                    )
                    current_leaks.append(leak_event)
                    self.leak_events.append(leak_event)
        
        return current_leaks


# ============================================================================
# ç»Ÿä¸€è°ƒè¯•æ¥å£
# ============================================================================

class MemoryDebugger:
    """ç»Ÿä¸€çš„å†…å­˜è°ƒè¯•å·¥å…·"""
    
    def __init__(self):
        self.profiler = None
        self.oom_detector = None
        self.leak_detector = None
        self.active_tools = []
        
    def start_monitoring(self, 
                        enable_profiler: bool = True,
                        enable_oom_detection: bool = True, 
                        enable_leak_detection: bool = True,
                        devices: Optional[List[int]] = None,
                        oom_threshold: float = 85.0):
        """å¼€å§‹å…¨é¢ç›‘æ§"""
        print("ğŸš€ å¯åŠ¨å†…å­˜è°ƒè¯•å·¥å…·...")
        
        if enable_profiler:
            self.profiler = MemoryProfiler(devices=devices)
            self.profiler.start_monitoring()
            self.active_tools.append(self.profiler)
        
        if enable_oom_detection:
            self.oom_detector = OOMDetector(threshold=oom_threshold)
            self.oom_detector.start_monitoring(devices=devices)
            self.active_tools.append(self.oom_detector)
        
        if enable_leak_detection:
            self.leak_detector = MemoryLeakDetector()
            self.leak_detector.start_monitoring(devices=devices)
            self.active_tools.append(self.leak_detector)
        
        print(f"âœ… å·²å¯åŠ¨ {len(self.active_tools)} ä¸ªç›‘æ§å·¥å…·")
    
    def stop_monitoring(self):
        """åœæ­¢æ‰€æœ‰ç›‘æ§"""
        print("ğŸ”„ åœæ­¢æ‰€æœ‰å†…å­˜ç›‘æ§å·¥å…·...")
        for tool in self.active_tools:
            if hasattr(tool, 'stop_monitoring'):
                tool.stop_monitoring()
        self.active_tools.clear()
        print("âœ… æ‰€æœ‰ç›‘æ§å·¥å…·å·²åœæ­¢")
    
    def get_status_report(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'active_tools': len(self.active_tools)
        }
        
        if self.profiler:
            report['memory_status'] = self.profiler.get_current_status()
        
        if self.leak_detector:
            recent_leaks = [event for event in self.leak_detector.leak_events 
                          if time.time() - event.timestamp < 3600]  # æœ€è¿‘1å°æ—¶
            report['recent_leaks'] = len(recent_leaks)
        
        return report


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def create_cli() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="Tiny Torch Memory Debug Tools - æ˜¾å­˜è°ƒè¯•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s profile --duration 60 --output report.json    # 60ç§’å†…å­˜åˆ†æ
  %(prog)s oom --threshold 85 --monitor                   # OOMç›‘æ§
  %(prog)s leak --check                                   # æ£€æŸ¥å†…å­˜æ³„æ¼
  %(prog)s monitor --all                                  # å¯åŠ¨å…¨é¢ç›‘æ§
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # Profile å‘½ä»¤
    profile_parser = subparsers.add_parser('profile', help='å†…å­˜åˆ†æ')
    profile_parser.add_argument('--duration', type=int, default=60, help='ç›‘æ§æ—¶é•¿(ç§’)')
    profile_parser.add_argument('--devices', type=int, nargs='+', help='ç›‘æ§çš„GPUè®¾å¤‡ID')
    profile_parser.add_argument('--output', default='memory_profile.json', help='è¾“å‡ºæ–‡ä»¶')
    profile_parser.add_argument('--interval', type=float, default=0.1, help='é‡‡æ ·é—´éš”(ç§’)')
    profile_parser.add_argument('--plot', action='store_true', help='ç”Ÿæˆå›¾è¡¨')
    
    # OOM å‘½ä»¤
    oom_parser = subparsers.add_parser('oom', help='OOMæ£€æµ‹')
    oom_parser.add_argument('--threshold', type=float, default=85.0, help='è­¦å‘Šé˜ˆå€¼(%)')
    oom_parser.add_argument('--monitor', action='store_true', help='æŒç»­ç›‘æ§æ¨¡å¼')
    oom_parser.add_argument('--devices', type=int, nargs='+', help='ç›‘æ§çš„GPUè®¾å¤‡ID')
    
    # Leak å‘½ä»¤
    leak_parser = subparsers.add_parser('leak', help='å†…å­˜æ³„æ¼æ£€æµ‹')
    leak_parser.add_argument('--check', action='store_true', help='æ‰§è¡Œæ³„æ¼æ£€æŸ¥')
    leak_parser.add_argument('--threshold', type=float, default=100, help='æ³„æ¼é˜ˆå€¼(MB)')
    leak_parser.add_argument('--devices', type=int, nargs='+', help='æ£€æŸ¥çš„GPUè®¾å¤‡ID')
    
    # Monitor å‘½ä»¤  
    monitor_parser = subparsers.add_parser('monitor', help='å…¨é¢ç›‘æ§')
    monitor_parser.add_argument('--all', action='store_true', help='å¯ç”¨æ‰€æœ‰ç›‘æ§åŠŸèƒ½')
    monitor_parser.add_argument('--duration', type=int, default=0, help='ç›‘æ§æ—¶é•¿(ç§’ï¼Œ0ä¸ºæ— é™)')
    monitor_parser.add_argument('--devices', type=int, nargs='+', help='ç›‘æ§çš„GPUè®¾å¤‡ID')
    
    return parser


def main():
    """ä¸»å‡½æ•°"""
    parser = create_cli()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # è®¾ç½®ä¿¡å·å¤„ç†
    stop_event = threading.Event()
    
    def signal_handler(signum, frame):
        print("\nğŸ”„ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        stop_event.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        if args.command == 'profile':
            profiler = MemoryProfiler(
                devices=args.devices,
                sampling_interval=args.interval
            )
            profiler.start_monitoring()
            
            print(f"ğŸ“Š å¼€å§‹å†…å­˜åˆ†æ (æ—¶é•¿: {args.duration}ç§’)")
            
            if args.duration > 0:
                for i in range(args.duration):
                    if stop_event.wait(1):
                        break
                    if (i + 1) % 10 == 0:
                        print(f"   è¿›åº¦: {i + 1}/{args.duration}ç§’")
            else:
                print("ğŸ“Š æŒç»­ç›‘æ§ä¸­ (æŒ‰Ctrl+Cåœæ­¢)...")
                stop_event.wait()
            
            profiler.stop_monitoring()
            report = profiler.generate_report(args.output)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {args.output}")
            
            # æ˜¾ç¤ºæ‘˜è¦
            if 'summary' in report:
                summary = report['summary']
                print(f"\nğŸ“ˆ ç›‘æ§æ‘˜è¦:")
                print(f"   è®¾å¤‡æ•°: {summary['total_devices']}")
                print(f"   æ€»æ˜¾å­˜: {summary['total_memory_gb']:.2f} GB")
                print(f"   å·²ä½¿ç”¨: {summary['total_used_gb']:.2f} GB")
                print(f"   åˆ©ç”¨ç‡: {summary['overall_utilization']:.1f}%")
        
        elif args.command == 'oom':
            detector = OOMDetector(threshold=args.threshold)
            detector.start_monitoring(devices=args.devices)
            
            if args.monitor:
                print(f"ğŸš¨ OOMç›‘æ§ä¸­ (é˜ˆå€¼: {args.threshold}%) - æŒ‰Ctrl+Cåœæ­¢")
                stop_event.wait()
            else:
                print("ğŸš¨ OOMæ£€æµ‹è¿è¡Œ5åˆ†é’Ÿ...")
                stop_event.wait(300)
            
            detector.stop_monitoring()
        
        elif args.command == 'leak':
            detector = MemoryLeakDetector(
                leak_threshold=args.threshold * 1024 * 1024
            )
            detector.start_monitoring(devices=args.devices)
            
            if args.check:
                print("ğŸ” æ£€æŸ¥å†…å­˜æ³„æ¼...")
                time.sleep(5)  # ç­‰å¾…ä¸€æ®µæ—¶é—´è§‚å¯Ÿ
                leaks = detector.check_for_leaks()
                
                if leaks:
                    print(f"âš ï¸  å‘ç° {len(leaks)} ä¸ªæ½œåœ¨å†…å­˜æ³„æ¼:")
                    for leak in leaks:
                        print(f"   GPU {leak.device_id}: {leak.leaked_memory/(1024**2):.1f} MB")
                        print(f"   ä¸¥é‡ç¨‹åº¦: {leak.severity}")
                else:
                    print("âœ… æœªå‘ç°æ˜æ˜¾çš„å†…å­˜æ³„æ¼")
            
            detector.stop_monitoring()
        
        elif args.command == 'monitor':
            debugger = MemoryDebugger()
            debugger.start_monitoring(devices=args.devices)
            
            duration = args.duration if args.duration > 0 else float('inf')
            print(f"ğŸ” å…¨é¢ç›‘æ§ä¸­ - æŒ‰Ctrl+Cåœæ­¢")
            
            start_time = time.time()
            while time.time() - start_time < duration:
                if stop_event.wait(10):
                    break
                
                # æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
                report = debugger.get_status_report()
                print(f"ğŸ“Š çŠ¶æ€: {report.get('active_tools', 0)} ä¸ªå·¥å…·è¿è¡Œä¸­")
            
            debugger.stop_monitoring()
    
    except KeyboardInterrupt:
        print("\nğŸ”„ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
