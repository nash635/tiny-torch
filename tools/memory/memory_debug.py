"""
Memory Debug Tools for Tiny Torch - ÊòæÂ≠òË∞ÉËØïÂ∑•ÂÖ∑ÈõÜ
Êï¥Âêà‰∫ÜÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÂú∫ÊôØÁöÑÊòæÂ≠òË∞ÉËØïÂäüËÉΩ

‰∏ªË¶ÅÂäüËÉΩ:
1. ÊòæÂ≠ò‰ΩøÁî®ÁõëÊéßÂíåÂàÜÊûê (Memory Profiler)
2. OOMÊ£ÄÊµãÂíåÈ¢ÑË≠¶ (OOM Detector) 
3. ÊòæÂ≠òÁ¢éÁâáÂàÜÊûê (Fragmentation Analyzer)
4. ÂÜÖÂ≠òÊ≥ÑÊºèÊ£ÄÊµã (Memory Leak Detector)
5. ÂàÜÂ∏ÉÂºèÂÜÖÂ≠òÁõëÊéß (Distributed Memory Monitor)
6. Áªü‰∏ÄÁöÑÂëΩ‰ª§Ë°åÁïåÈù¢ (CLI)

‰ΩøÁî®Á§∫‰æã:
    from tools.memory import MemoryDebugger
    
    # Âü∫Êú¨ÁõëÊéß
    debugger = MemoryDebugger()
    debugger.start_monitoring()
    
    # ÂëΩ‰ª§Ë°å‰ΩøÁî®
    python -m tools.memory profile --duration 60
    python -m tools.memory oom --threshold 85
"""

import os
import gc
import time
import json
import threading
import socket
import pickle
import signal
import subprocess
import argparse
import sys
import traceback
import weakref
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import queue
import multiprocessing as mp

# Á¨¨‰∏âÊñπ‰æùËµñ
try:
    import numpy as np
    import matplotlib.pyplot as plt
    import psutil
except ImportError as e:
    print(f"Ë≠¶Âëä: Áº∫Â∞ë‰æùËµñÈ°π {e}. ËØ∑ËøêË°å: pip install numpy matplotlib psutil")
    sys.exit(1)


# ============================================================================
# Êï∞ÊçÆÁªìÊûÑÂÆö‰πâ
# ============================================================================

class OOMRiskLevel(Enum):
    """OOMÈ£éÈô©Á≠âÁ∫ß"""
    LOW = "low"
    MEDIUM = "medium" 
    HIGH = "high"
    CRITICAL = "critical"


class FragmentationLevel(Enum):
    """Á¢éÁâáÂåñÁ®ãÂ∫¶Á≠âÁ∫ß"""
    LOW = "low"
    MODERATE = "moderate"
    HIGH = "high"
    SEVERE = "severe"


class DistributedRole(Enum):
    """ÂàÜÂ∏ÉÂºèËßíËâ≤"""
    MASTER = "master"
    WORKER = "worker"
    STANDALONE = "standalone"


@dataclass
class MemorySnapshot:
    """ÊòæÂ≠òÂø´ÁÖßÊï∞ÊçÆÁªìÊûÑ"""
    timestamp: float
    device_id: int
    allocated: int  # Â∑≤ÂàÜÈÖçÊòæÂ≠ò (bytes)
    reserved: int   # È¢ÑÁïôÊòæÂ≠ò (bytes) 
    free: int       # ÂèØÁî®ÊòæÂ≠ò (bytes)
    total: int      # ÊÄªÊòæÂ≠ò (bytes)
    utilization: float  # Âà©Áî®Áéá (%)
    fragmentation: float  # Á¢éÁâáÁéá (%)
    process_info: Dict[str, Any] = field(default_factory=dict)


@dataclass
class OOMEvent:
    """OOM‰∫ã‰ª∂ËÆ∞ÂΩï"""
    timestamp: float
    device_id: int
    requested_memory: int
    available_memory: int
    total_memory: int
    stack_trace: Optional[str] = None
    context_info: Optional[Dict[str, Any]] = None


@dataclass
class OOMPrediction:
    """OOMÈ¢ÑÊµãÁªìÊûú"""
    device_id: int
    risk_level: OOMRiskLevel
    predicted_oom_time: Optional[float]
    confidence: float  # È¢ÑÊµãÁΩÆ‰ø°Â∫¶ (0-1)
    current_usage: float
    trend_slope: float
    recommended_actions: List[str] = field(default_factory=list)


@dataclass
class MemoryLeakEvent:
    """ÂÜÖÂ≠òÊ≥ÑÊºè‰∫ã‰ª∂"""
    timestamp: float
    device_id: int
    leaked_memory: int
    leak_rate: float
    potential_sources: List[str] = field(default_factory=list)
    stack_trace: Optional[str] = None
    severity: str = "medium"


# ============================================================================
# Ê†∏ÂøÉÂ∑•ÂÖ∑Á±ª
# ============================================================================

class MemoryProfiler:
    """ÊòæÂ≠ò‰ΩøÁî®ÂàÜÊûêÂô®"""
    
    def __init__(self, 
                 devices: Optional[List[int]] = None,
                 sampling_interval: float = 0.1,
                 max_snapshots: int = 10000):
        self.devices = devices or self._get_available_devices()
        self.sampling_interval = sampling_interval
        self.max_snapshots = max_snapshots
        self.snapshots: Dict[int, List[MemorySnapshot]] = defaultdict(list)
        self.monitoring = False
        self.monitor_thread = None
        
    def _get_available_devices(self) -> List[int]:
        """Ëé∑ÂèñÂèØÁî®ÁöÑGPUËÆæÂ§á"""
        try:
            result = subprocess.run(['nvidia-smi', '-L'], 
                                 capture_output=True, text=True, check=True)
            devices = []
            for line in result.stdout.strip().split('\n'):
                if 'GPU' in line:
                    device_id = int(line.split(':')[0].split()[-1])
                    devices.append(device_id)
            return devices
        except (subprocess.CalledProcessError, FileNotFoundError):
            return []
    
    def _get_memory_info(self, device_id: int) -> Optional[MemorySnapshot]:
        """Ëé∑ÂèñÊåáÂÆöËÆæÂ§áÁöÑÂÜÖÂ≠ò‰ø°ÊÅØ"""
        try:
            cmd = f"nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv,noheader,nounits -i {device_id}"
            result = subprocess.run(cmd.split(), capture_output=True, text=True, check=True)
            used, free, total = map(int, result.stdout.strip().split(', '))
            
            # ËΩ¨Êç¢‰∏∫Â≠óËäÇ
            used_bytes = used * 1024 * 1024
            free_bytes = free * 1024 * 1024 
            total_bytes = total * 1024 * 1024
            reserved_bytes = total_bytes - free_bytes
            
            utilization = (used_bytes / total_bytes) * 100 if total_bytes > 0 else 0
            fragmentation = self._calculate_fragmentation(device_id, free_bytes, total_bytes)
            
            return MemorySnapshot(
                timestamp=time.time(),
                device_id=device_id,
                allocated=used_bytes,
                reserved=reserved_bytes,
                free=free_bytes,
                total=total_bytes,
                utilization=utilization,
                fragmentation=fragmentation,
                process_info=self._get_process_info()
            )
        except Exception as e:
            print(f"Ë≠¶Âëä: Êó†Ê≥ïËé∑ÂèñËÆæÂ§á {device_id} ÁöÑÂÜÖÂ≠ò‰ø°ÊÅØ: {e}")
            return None
    
    def _calculate_fragmentation(self, device_id: int, free_bytes: int, total_bytes: int) -> float:
        """ËÆ°ÁÆóÂÜÖÂ≠òÁ¢éÁâáÁéáÔºàÁÆÄÂåñÁâàÊú¨Ôºâ"""
        if total_bytes == 0:
            return 0.0
        return max(0, min(100, (1 - free_bytes / total_bytes) * 50))
    
    def _get_process_info(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂΩìÂâçËøõÁ®ãÁöÑÂÜÖÂ≠ò‰ø°ÊÅØ"""
        try:
            process = psutil.Process()
            return {
                'pid': process.pid,
                'memory_percent': process.memory_percent(),
                'memory_info': process.memory_info()._asdict(),
                'cpu_percent': process.cpu_percent()
            }
        except:
            return {}
    
    def start_monitoring(self):
        """ÂºÄÂßãÁõëÊéß"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print(f"üìä ÂºÄÂßãÁõëÊéß {len(self.devices)} ‰∏™GPUËÆæÂ§á")
    
    def stop_monitoring(self):
        """ÂÅúÊ≠¢ÁõëÊéß"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        print("üîÑ ÂÅúÊ≠¢ÂÜÖÂ≠òÁõëÊéß")
    
    def _monitor_loop(self):
        """ÁõëÊéßÂæ™ÁéØ"""
        while self.monitoring:
            for device_id in self.devices:
                snapshot = self._get_memory_info(device_id)
                if snapshot:
                    self.snapshots[device_id].append(snapshot)
                    # ÈôêÂà∂Âø´ÁÖßÊï∞Èáè
                    if len(self.snapshots[device_id]) > self.max_snapshots:
                        self.snapshots[device_id].pop(0)
            
            time.sleep(self.sampling_interval)
    
    def get_current_status(self) -> Dict[int, Dict[str, Any]]:
        """Ëé∑ÂèñÂΩìÂâçÁä∂ÊÄÅ"""
        status = {}
        for device_id in self.devices:
            if device_id in self.snapshots and self.snapshots[device_id]:
                latest = self.snapshots[device_id][-1]
                status[device_id] = {
                    'current_used': latest.allocated,
                    'total': latest.total,
                    'utilization': latest.utilization,
                    'fragmentation': latest.fragmentation,
                    'free': latest.free
                }
        return status
    
    def generate_report(self, output_file: str) -> Dict[str, Any]:
        """ÁîüÊàêÂàÜÊûêÊä•Âëä"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'devices': {},
            'summary': {}
        }
        
        total_memory = 0
        total_used = 0
        
        for device_id, snapshots in self.snapshots.items():
            if not snapshots:
                continue
                
            latest = snapshots[-1]
            utilizations = [s.utilization for s in snapshots]
            
            device_report = {
                'device_id': device_id,
                'current_usage': {
                    'allocated': latest.allocated,
                    'free': latest.free,
                    'total': latest.total,
                    'utilization_percent': latest.utilization
                },
                'statistics': {
                    'avg_utilization': np.mean(utilizations),
                    'max_utilization': np.max(utilizations),
                    'min_utilization': np.min(utilizations),
                    'samples_count': len(snapshots)
                },
                'fragmentation': latest.fragmentation
            }
            
            report['devices'][device_id] = device_report
            total_memory += latest.total
            total_used += latest.allocated
        
        report['summary'] = {
            'total_devices': len(self.devices),
            'total_memory_gb': total_memory / (1024**3),
            'total_used_gb': total_used / (1024**3),
            'overall_utilization': (total_used / total_memory * 100) if total_memory > 0 else 0
        }
        
        # ‰øùÂ≠òÊä•Âëä
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report


class OOMDetector:
    """OOMÊ£ÄÊµãÂíåÈ¢ÑË≠¶Âô®"""
    
    def __init__(self, 
                 threshold: float = 85.0,
                 prediction_window: int = 10,
                 warning_callback: Optional[Callable] = None):
        self.threshold = threshold
        self.prediction_window = prediction_window
        self.warning_callback = warning_callback or self._default_warning_callback
        self.monitoring = False
        self.monitor_thread = None
        self.memory_history: Dict[int, List[Tuple[float, float]]] = defaultdict(list)
        
    def _default_warning_callback(self, prediction: OOMPrediction):
        """ÈªòËÆ§Ë≠¶ÂëäÂõûË∞É"""
        print(f"‚ö†Ô∏è  OOMË≠¶Âëä: GPU {prediction.device_id}")
        print(f"   È£éÈô©Á≠âÁ∫ß: {prediction.risk_level.value}")
        print(f"   ÂΩìÂâç‰ΩøÁî®Áéá: {prediction.current_usage:.1f}%")
        if prediction.predicted_oom_time:
            print(f"   È¢ÑÊµãOOMÊó∂Èó¥: {prediction.predicted_oom_time:.1f}ÁßíÂêé")
        print(f"   ÁΩÆ‰ø°Â∫¶: {prediction.confidence:.2f}")
        for action in prediction.recommended_actions:
            print(f"   Âª∫ËÆÆ: {action}")
    
    def start_monitoring(self, devices: Optional[List[int]] = None):
        """ÂºÄÂßãOOMÁõëÊéß"""
        if self.monitoring:
            return
            
        self.devices = devices or self._get_available_devices()
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print(f"üö® ÂºÄÂßãOOMÁõëÊéß (ÈòàÂÄº: {self.threshold}%)")
    
    def stop_monitoring(self):
        """ÂÅúÊ≠¢ÁõëÊéß"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        print("üîÑ ÂÅúÊ≠¢OOMÁõëÊéß")
    
    def _get_available_devices(self) -> List[int]:
        """Ëé∑ÂèñÂèØÁî®ËÆæÂ§á"""
        try:
            result = subprocess.run(['nvidia-smi', '-L'], 
                                 capture_output=True, text=True, check=True)
            devices = []
            for line in result.stdout.strip().split('\n'):
                if 'GPU' in line:
                    device_id = int(line.split(':')[0].split()[-1])
                    devices.append(device_id)
            return devices
        except:
            return []
    
    def _monitor_loop(self):
        """ÁõëÊéßÂæ™ÁéØ"""
        while self.monitoring:
            for device_id in self.devices:
                utilization = self._get_device_utilization(device_id)
                if utilization is not None:
                    self.memory_history[device_id].append((time.time(), utilization))
                    
                    # ÈôêÂà∂ÂéÜÂè≤ËÆ∞ÂΩïÈïøÂ∫¶
                    if len(self.memory_history[device_id]) > 100:
                        self.memory_history[device_id].pop(0)
                    
                    # ËøõË°åOOMÈ¢ÑÊµã
                    prediction = self._predict_oom(device_id)
                    if prediction and prediction.risk_level in [OOMRiskLevel.HIGH, OOMRiskLevel.CRITICAL]:
                        self.warning_callback(prediction)
            
            time.sleep(1.0)
    
    def _get_device_utilization(self, device_id: int) -> Optional[float]:
        """Ëé∑ÂèñËÆæÂ§áÂà©Áî®Áéá"""
        try:
            cmd = f"nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits -i {device_id}"
            result = subprocess.run(cmd.split(), capture_output=True, text=True, check=True)
            used, total = map(int, result.stdout.strip().split(', '))
            return (used / total) * 100 if total > 0 else 0
        except:
            return None
    
    def _predict_oom(self, device_id: int) -> Optional[OOMPrediction]:
        """È¢ÑÊµãOOMÈ£éÈô©"""
        history = self.memory_history.get(device_id, [])
        if len(history) < 3:
            return None
        
        recent_usage = [usage for _, usage in history[-self.prediction_window:]]
        current_usage = recent_usage[-1]
        
        # ËÆ°ÁÆóË∂ãÂäø
        times = list(range(len(recent_usage)))
        if len(times) > 1:
            trend_slope = np.polyfit(times, recent_usage, 1)[0]
        else:
            trend_slope = 0
        
        # ËØÑ‰º∞È£éÈô©
        risk_level = OOMRiskLevel.LOW
        predicted_time = None
        confidence = 0.5
        actions = []
        
        if current_usage > 95:
            risk_level = OOMRiskLevel.CRITICAL
            confidence = 0.95
            actions = ["Á´ãÂç≥ÈáäÊîæÁºìÂ≠ò: tiny_torch.cuda.empty_cache()", "ÂáèÂ∞ëbatch size", "Ê£ÄÊü•ÊòØÂê¶ÊúâÂÜÖÂ≠òÊ≥ÑÊºè"]
        elif current_usage > self.threshold:
            if trend_slope > 1.0:
                risk_level = OOMRiskLevel.HIGH
                predicted_time = (100 - current_usage) / trend_slope
                confidence = 0.8
                actions = ["ÂáÜÂ§áÈáäÊîæÁºìÂ≠ò", "ËÄÉËôëÂáèÂ∞ëbatch size"]
            else:
                risk_level = OOMRiskLevel.MEDIUM
                confidence = 0.6
                actions = ["ÁõëÊéßÂÜÖÂ≠ò‰ΩøÁî®Ë∂ãÂäø"]
        
        return OOMPrediction(
            device_id=device_id,
            risk_level=risk_level,
            predicted_oom_time=predicted_time,
            confidence=confidence,
            current_usage=current_usage,
            trend_slope=trend_slope,
            recommended_actions=actions
        )


class MemoryLeakDetector:
    """ÂÜÖÂ≠òÊ≥ÑÊºèÊ£ÄÊµãÂô®"""
    
    def __init__(self, leak_threshold: float = 100 * 1024 * 1024):  # 100MB
        self.leak_threshold = leak_threshold
        self.monitoring = False
        self.baseline_memory: Dict[int, int] = {}
        self.leak_events: List[MemoryLeakEvent] = []
        
    def start_monitoring(self, devices: Optional[List[int]] = None):
        """ÂºÄÂßãÊ≥ÑÊºèÊ£ÄÊµã"""
        self.devices = devices or self._get_available_devices()
        
        # Âª∫Á´ãÂü∫Á∫ø
        for device_id in self.devices:
            usage = self._get_device_memory_usage(device_id)
            if usage is not None:
                self.baseline_memory[device_id] = usage
        
        self.monitoring = True
        print("üîç ÂºÄÂßãÂÜÖÂ≠òÊ≥ÑÊºèÊ£ÄÊµã")
    
    def stop_monitoring(self):
        """ÂÅúÊ≠¢Ê£ÄÊµã"""
        self.monitoring = False
        print("üîÑ ÂÅúÊ≠¢ÂÜÖÂ≠òÊ≥ÑÊºèÊ£ÄÊµã")
    
    def _get_available_devices(self) -> List[int]:
        """Ëé∑ÂèñÂèØÁî®ËÆæÂ§á"""
        try:
            result = subprocess.run(['nvidia-smi', '-L'], 
                                 capture_output=True, text=True, check=True)
            devices = []
            for line in result.stdout.strip().split('\n'):
                if 'GPU' in line:
                    device_id = int(line.split(':')[0].split()[-1])
                    devices.append(device_id)
            return devices
        except:
            return []
    
    def _get_device_memory_usage(self, device_id: int) -> Optional[int]:
        """Ëé∑ÂèñËÆæÂ§áÂÜÖÂ≠ò‰ΩøÁî®Èáè"""
        try:
            cmd = f"nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i {device_id}"
            result = subprocess.run(cmd.split(), capture_output=True, text=True, check=True)
            return int(result.stdout.strip()) * 1024 * 1024  # ËΩ¨Êç¢‰∏∫Â≠óËäÇ
        except:
            return None
    
    def check_for_leaks(self) -> List[MemoryLeakEvent]:
        """Ê£ÄÊü•ÂÜÖÂ≠òÊ≥ÑÊºè"""
        if not self.monitoring:
            return []
        
        current_leaks = []
        
        for device_id in self.devices:
            current_usage = self._get_device_memory_usage(device_id)
            baseline = self.baseline_memory.get(device_id)
            
            if current_usage is not None and baseline is not None:
                memory_increase = current_usage - baseline
                
                if memory_increase > self.leak_threshold:
                    leak_event = MemoryLeakEvent(
                        timestamp=time.time(),
                        device_id=device_id,
                        leaked_memory=memory_increase,
                        leak_rate=memory_increase / 3600,  # ÁÆÄÂåñËÆ°ÁÆó
                        potential_sources=["Êú™Áü•"],
                        severity="high" if memory_increase > self.leak_threshold * 2 else "medium"
                    )
                    current_leaks.append(leak_event)
                    self.leak_events.append(leak_event)
        
        return current_leaks


# ============================================================================
# Áªü‰∏ÄË∞ÉËØïÊé•Âè£
# ============================================================================

class MemoryDebugger:
    """Áªü‰∏ÄÁöÑÂÜÖÂ≠òË∞ÉËØïÂ∑•ÂÖ∑"""
    
    def __init__(self):
        self.profiler = None
        self.oom_detector = None
        self.leak_detector = None
        self.active_tools = []
        
    def start_monitoring(self, 
                        enable_profiler: bool = True,
                        enable_oom_detection: bool = True, 
                        enable_leak_detection: bool = True,
                        devices: Optional[List[int]] = None,
                        oom_threshold: float = 85.0):
        """ÂºÄÂßãÂÖ®Èù¢ÁõëÊéß"""
        print("üöÄ ÂêØÂä®ÂÜÖÂ≠òË∞ÉËØïÂ∑•ÂÖ∑...")
        
        if enable_profiler:
            self.profiler = MemoryProfiler(devices=devices)
            self.profiler.start_monitoring()
            self.active_tools.append(self.profiler)
        
        if enable_oom_detection:
            self.oom_detector = OOMDetector(threshold=oom_threshold)
            self.oom_detector.start_monitoring(devices=devices)
            self.active_tools.append(self.oom_detector)
        
        if enable_leak_detection:
            self.leak_detector = MemoryLeakDetector()
            self.leak_detector.start_monitoring(devices=devices)
            self.active_tools.append(self.leak_detector)
        
        print(f"‚úÖ Â∑≤ÂêØÂä® {len(self.active_tools)} ‰∏™ÁõëÊéßÂ∑•ÂÖ∑")
    
    def stop_monitoring(self):
        """ÂÅúÊ≠¢ÊâÄÊúâÁõëÊéß"""
        print("üîÑ ÂÅúÊ≠¢ÊâÄÊúâÂÜÖÂ≠òÁõëÊéßÂ∑•ÂÖ∑...")
        for tool in self.active_tools:
            if hasattr(tool, 'stop_monitoring'):
                tool.stop_monitoring()
        self.active_tools.clear()
        print("‚úÖ ÊâÄÊúâÁõëÊéßÂ∑•ÂÖ∑Â∑≤ÂÅúÊ≠¢")
    
    def get_status_report(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁä∂ÊÄÅÊä•Âëä"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'active_tools': len(self.active_tools)
        }
        
        if self.profiler:
            report['memory_status'] = self.profiler.get_current_status()
        
        if self.leak_detector:
            recent_leaks = [event for event in self.leak_detector.leak_events 
                          if time.time() - event.timestamp < 3600]  # ÊúÄËøë1Â∞èÊó∂
            report['recent_leaks'] = len(recent_leaks)
        
        return report


# ============================================================================
# ÂëΩ‰ª§Ë°åÊé•Âè£
# ============================================================================

def create_cli() -> argparse.ArgumentParser:
    """ÂàõÂª∫ÂëΩ‰ª§Ë°åËß£ÊûêÂô®"""
    parser = argparse.ArgumentParser(
        description="Tiny Torch Memory Debug Tools - ÊòæÂ≠òË∞ÉËØïÂ∑•ÂÖ∑",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
‰ΩøÁî®Á§∫‰æã:
  %(prog)s profile --duration 60 --output report.json    # 60ÁßíÂÜÖÂ≠òÂàÜÊûê
  %(prog)s oom --threshold 85 --monitor                   # OOMÁõëÊéß
  %(prog)s leak --check                                   # Ê£ÄÊü•ÂÜÖÂ≠òÊ≥ÑÊºè
  %(prog)s monitor --all                                  # ÂêØÂä®ÂÖ®Èù¢ÁõëÊéß
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ÂèØÁî®ÂëΩ‰ª§')
    
    # Profile ÂëΩ‰ª§
    profile_parser = subparsers.add_parser('profile', help='ÂÜÖÂ≠òÂàÜÊûê')
    profile_parser.add_argument('--duration', type=int, default=60, help='ÁõëÊéßÊó∂Èïø(Áßí)')
    profile_parser.add_argument('--devices', type=int, nargs='+', help='ÁõëÊéßÁöÑGPUËÆæÂ§áID')
    profile_parser.add_argument('--output', default='memory_profile.json', help='ËæìÂá∫Êñá‰ª∂')
    profile_parser.add_argument('--interval', type=float, default=0.1, help='ÈááÊ†∑Èó¥Èöî(Áßí)')
    profile_parser.add_argument('--plot', action='store_true', help='ÁîüÊàêÂõæË°®')
    
    # OOM ÂëΩ‰ª§
    oom_parser = subparsers.add_parser('oom', help='OOMÊ£ÄÊµã')
    oom_parser.add_argument('--threshold', type=float, default=85.0, help='Ë≠¶ÂëäÈòàÂÄº(%)')
    oom_parser.add_argument('--monitor', action='store_true', help='ÊåÅÁª≠ÁõëÊéßÊ®°Âºè')
    oom_parser.add_argument('--devices', type=int, nargs='+', help='ÁõëÊéßÁöÑGPUËÆæÂ§áID')
    
    # Leak ÂëΩ‰ª§
    leak_parser = subparsers.add_parser('leak', help='ÂÜÖÂ≠òÊ≥ÑÊºèÊ£ÄÊµã')
    leak_parser.add_argument('--check', action='store_true', help='ÊâßË°åÊ≥ÑÊºèÊ£ÄÊü•')
    leak_parser.add_argument('--threshold', type=float, default=100, help='Ê≥ÑÊºèÈòàÂÄº(MB)')
    leak_parser.add_argument('--devices', type=int, nargs='+', help='Ê£ÄÊü•ÁöÑGPUËÆæÂ§áID')
    
    # Monitor ÂëΩ‰ª§  
    monitor_parser = subparsers.add_parser('monitor', help='ÂÖ®Èù¢ÁõëÊéß')
    monitor_parser.add_argument('--all', action='store_true', help='ÂêØÁî®ÊâÄÊúâÁõëÊéßÂäüËÉΩ')
    monitor_parser.add_argument('--duration', type=int, default=0, help='ÁõëÊéßÊó∂Èïø(ÁßíÔºå0‰∏∫Êó†Èôê)')
    monitor_parser.add_argument('--devices', type=int, nargs='+', help='ÁõëÊéßÁöÑGPUËÆæÂ§áID')
    
    return parser


def main():
    """‰∏ªÂáΩÊï∞"""
    parser = create_cli()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # ËÆæÁΩÆ‰ø°Âè∑Â§ÑÁêÜ
    stop_event = threading.Event()
    
    def signal_handler(signum, frame):
        print("\nüîÑ Êî∂Âà∞ÈÄÄÂá∫‰ø°Âè∑ÔºåÊ≠£Âú®ÂÅúÊ≠¢...")
        stop_event.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        if args.command == 'profile':
            profiler = MemoryProfiler(
                devices=args.devices,
                sampling_interval=args.interval
            )
            profiler.start_monitoring()
            
            print(f"üìä ÂºÄÂßãÂÜÖÂ≠òÂàÜÊûê (Êó∂Èïø: {args.duration}Áßí)")
            
            if args.duration > 0:
                for i in range(args.duration):
                    if stop_event.wait(1):
                        break
                    if (i + 1) % 10 == 0:
                        print(f"   ËøõÂ∫¶: {i + 1}/{args.duration}Áßí")
            else:
                print("üìä ÊåÅÁª≠ÁõëÊéß‰∏≠ (ÊåâCtrl+CÂÅúÊ≠¢)...")
                stop_event.wait()
            
            profiler.stop_monitoring()
            report = profiler.generate_report(args.output)
            print(f"üìÑ Êä•ÂëäÂ∑≤‰øùÂ≠ò: {args.output}")
            
            # ÊòæÁ§∫ÊëòË¶Å
            if 'summary' in report:
                summary = report['summary']
                print(f"\nüìà ÁõëÊéßÊëòË¶Å:")
                print(f"   ËÆæÂ§áÊï∞: {summary['total_devices']}")
                print(f"   ÊÄªÊòæÂ≠ò: {summary['total_memory_gb']:.2f} GB")
                print(f"   Â∑≤‰ΩøÁî®: {summary['total_used_gb']:.2f} GB")
                print(f"   Âà©Áî®Áéá: {summary['overall_utilization']:.1f}%")
        
        elif args.command == 'oom':
            detector = OOMDetector(threshold=args.threshold)
            detector.start_monitoring(devices=args.devices)
            
            if args.monitor:
                print(f"üö® OOMÁõëÊéß‰∏≠ (ÈòàÂÄº: {args.threshold}%) - ÊåâCtrl+CÂÅúÊ≠¢")
                stop_event.wait()
            else:
                print("üö® OOMÊ£ÄÊµãËøêË°å5ÂàÜÈíü...")
                stop_event.wait(300)
            
            detector.stop_monitoring()
        
        elif args.command == 'leak':
            detector = MemoryLeakDetector(
                leak_threshold=args.threshold * 1024 * 1024
            )
            detector.start_monitoring(devices=args.devices)
            
            if args.check:
                print("üîç Ê£ÄÊü•ÂÜÖÂ≠òÊ≥ÑÊºè...")
                time.sleep(5)  # Á≠âÂæÖ‰∏ÄÊÆµÊó∂Èó¥ËßÇÂØü
                leaks = detector.check_for_leaks()
                
                if leaks:
                    print(f"‚ö†Ô∏è  ÂèëÁé∞ {len(leaks)} ‰∏™ÊΩúÂú®ÂÜÖÂ≠òÊ≥ÑÊºè:")
                    for leak in leaks:
                        print(f"   GPU {leak.device_id}: {leak.leaked_memory/(1024**2):.1f} MB")
                        print(f"   ‰∏•ÈáçÁ®ãÂ∫¶: {leak.severity}")
                else:
                    print("‚úÖ Êú™ÂèëÁé∞ÊòéÊòæÁöÑÂÜÖÂ≠òÊ≥ÑÊºè")
            
            detector.stop_monitoring()
        
        elif args.command == 'monitor':
            debugger = MemoryDebugger()
            debugger.start_monitoring(devices=args.devices)
            
            duration = args.duration if args.duration > 0 else float('inf')
            print(f"üîç ÂÖ®Èù¢ÁõëÊéß‰∏≠ - ÊåâCtrl+CÂÅúÊ≠¢")
            
            start_time = time.time()
            while time.time() - start_time < duration:
                if stop_event.wait(10):
                    break
                
                # ÊØè10ÁßíÊòæÁ§∫‰∏ÄÊ¨°Áä∂ÊÄÅ
                report = debugger.get_status_report()
                print(f"üìä Áä∂ÊÄÅ: {report.get('active_tools', 0)} ‰∏™Â∑•ÂÖ∑ËøêË°å‰∏≠")
            
            debugger.stop_monitoring()
    
    except KeyboardInterrupt:
        print("\nüîÑ Áî®Êà∑‰∏≠Êñ≠ÔºåÊ≠£Âú®ÈÄÄÂá∫...")
    except Exception as e:
        print(f"‚ùå ÈîôËØØ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
