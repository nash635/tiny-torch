# Tiny-Torch: A PyTorch-Inspired Deep Learning Framework

![Tiny-Torch](https://img.shields.io/badge/Tiny--Torch-v0.1.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.0%2B-green.svg)
![C++](https://img.shields.io/badge/C%2B%2B-17-orange.svg)

Tiny-Torch æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹å®ç°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸¥æ ¼å‚è€ƒ [PyTorch](https://github.com/pytorch/pytorch) çš„æ¶æ„è®¾è®¡å’Œåº•å±‚å®ç°ã€‚æœ¬é¡¹ç›®æ—¨åœ¨é€šè¿‡é‡æ–°å®ç° PyTorch çš„æ ¸å¿ƒç»„ä»¶ï¼Œæ·±å…¥ç†è§£ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶çš„åº•å±‚æœºåˆ¶ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

- **ä¸¥æ ¼å‚è€ƒPyTorch**: éµå¾ªPyTorchçš„APIè®¾è®¡å’Œå®ç°æ¨¡å¼
- **åº•å±‚ä¼˜åŒ–**: æ ¸å¿ƒç®—å­ä½¿ç”¨C++/CUDAå®ç°ï¼Œç¡®ä¿æ€§èƒ½
- **æ•™è‚²å¯¼å‘**: æä¾›æ¸…æ™°çš„ä»£ç æ³¨é‡Šå’Œå®ç°æ–‡æ¡£
- **æ¨¡å—åŒ–è®¾è®¡**: é‡‡ç”¨PyTorchçš„åˆ†å±‚æ¶æ„ï¼Œä¾¿äºå­¦ä¹ å’Œæ‰©å±•

## ğŸ“ é¡¹ç›®ç»“æ„

```
tiny-torch/
â”œâ”€â”€ README.md
â”œâ”€â”€ CMakeLists.txt              # CMakeæ„å»ºé…ç½®
â”œâ”€â”€ setup.py                   # PythonåŒ…å®‰è£…é…ç½®
â”œâ”€â”€ pyproject.toml             # ç°ä»£Pythoné¡¹ç›®é…ç½®
â”œâ”€â”€ requirements.txt           # Pythonä¾èµ–
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .clang-format              # C++ä»£ç æ ¼å¼åŒ–
â”œâ”€â”€ csrc/                      # C++/CUDAæºç  (å‚è€ƒpytorch/csrc)
â”‚   â”œâ”€â”€ api/                   # Python C APIç»‘å®š
â”‚   â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”œâ”€â”€ aten/                  # ATenå¼ é‡åº“ (å‚è€ƒpytorch/aten)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ ATen/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ core/       # æ ¸å¿ƒå¼ é‡ç±»
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ native/     # CPUå®ç°
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cuda/       # CUDAå®ç°
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ops/        # ç®—å­å®šä¹‰
â”‚   â”‚   â”‚   â””â”€â”€ TH/             # åº•å±‚å¼ é‡å®ç°
â”‚   â”‚   â””â”€â”€ include/
â”‚   â”œâ”€â”€ autograd/              # è‡ªåŠ¨å¾®åˆ†å¼•æ“
â”‚   â”‚   â”œâ”€â”€ engine.cpp
â”‚   â”‚   â”œâ”€â”€ function.cpp
â”‚   â”‚   â””â”€â”€ variable.cpp
â”‚   â””â”€â”€ jit/                   # å³æ—¶ç¼–è¯‘ (ç®€åŒ–ç‰ˆ)
â”‚       â”œâ”€â”€ codegen/
â”‚       â””â”€â”€ passes/
â”œâ”€â”€ torch/                     # Pythonå‰ç«¯ (å‚è€ƒpytorch/torch)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ _C/                    # Cæ‰©å±•ç»‘å®š
â”‚   â”œâ”€â”€ nn/                    # ç¥ç»ç½‘ç»œæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ modules/           # ç½‘ç»œå±‚å®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ module.py      # åŸºç¡€Moduleç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ linear.py      # çº¿æ€§å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ conv.py        # å·ç§¯å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ activation.py  # æ¿€æ´»å‡½æ•°
â”‚   â”‚   â”‚   â”œâ”€â”€ loss.py        # æŸå¤±å‡½æ•°
â”‚   â”‚   â”‚   â”œâ”€â”€ batchnorm.py   # æ‰¹å½’ä¸€åŒ–
â”‚   â”‚   â”‚   â””â”€â”€ container.py   # å®¹å™¨ç±»
â”‚   â”‚   â”œâ”€â”€ functional.py      # å‡½æ•°å¼æ¥å£
â”‚   â”‚   â”œâ”€â”€ init.py           # å‚æ•°åˆå§‹åŒ–
â”‚   â”‚   â””â”€â”€ parameter.py       # å‚æ•°ç±»
â”‚   â”œâ”€â”€ optim/                 # ä¼˜åŒ–å™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ optimizer.py       # åŸºç¡€ä¼˜åŒ–å™¨
â”‚   â”‚   â”œâ”€â”€ sgd.py            # SGD
â”‚   â”‚   â”œâ”€â”€ adam.py           # Adam
â”‚   â”‚   â””â”€â”€ lr_scheduler.py    # å­¦ä¹ ç‡è°ƒåº¦
â”‚   â”œâ”€â”€ autograd/             # è‡ªåŠ¨å¾®åˆ†Pythonæ¥å£
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ function.py       # FunctionåŸºç±»
â”‚   â”‚   â”œâ”€â”€ variable.py       # Variable(å·²åºŸå¼ƒ,å…¼å®¹æ€§)
â”‚   â”‚   â””â”€â”€ functional.py     # è‡ªåŠ¨å¾®åˆ†å‡½æ•°
â”‚   â”œâ”€â”€ utils/                # å®ç”¨å·¥å…·
â”‚   â”‚   â”œâ”€â”€ data/             # æ•°æ®å¤„ç†
â”‚   â”‚   â””â”€â”€ cpp_extension.py  # C++æ‰©å±•å·¥å…·
â”‚   â””â”€â”€ distributed/          # åˆ†å¸ƒå¼è®­ç»ƒ
â”œâ”€â”€ test/                     # æµ‹è¯•å¥—ä»¶
â”‚   â”œâ”€â”€ test_tensor.py
â”‚   â”œâ”€â”€ test_autograd.py
â”‚   â”œâ”€â”€ test_nn.py
â”‚   â”œâ”€â”€ test_optim.py
â”‚   â””â”€â”€ cpp/                  # C++æµ‹è¯•
â”œâ”€â”€ benchmarks/               # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ tensor_ops.py
â”‚   â”œâ”€â”€ autograd_benchmark.py
â”‚   â””â”€â”€ compare_with_pytorch.py
â”œâ”€â”€ examples/                 # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ mnist/
â”‚   â”œâ”€â”€ cifar10/
â”‚   â””â”€â”€ transformer/
â”œâ”€â”€ docs/                     # æ–‡æ¡£
â”‚   â”œâ”€â”€ design/               # è®¾è®¡æ–‡æ¡£
â”‚   â”œâ”€â”€ api/                  # APIæ–‡æ¡£
â”‚   â””â”€â”€ tutorials/            # æ•™ç¨‹
â””â”€â”€ tools/                    # æ„å»ºå’Œå¼€å‘å·¥å…·
    â”œâ”€â”€ build/
    â”œâ”€â”€ setup_helpers/
    â””â”€â”€ codegen/              # ä»£ç ç”Ÿæˆå·¥å…·
```

## ğŸš€ å®ç°é˜¶æ®µ

### Phase 1: æ ¸å¿ƒåŸºç¡€è®¾æ–½ (Week 1-3)

#### 1.1 æ„å»ºç³»ç»Ÿè®¾ç½®
```bash
# å‚è€ƒ pytorch/CMakeLists.txt å’Œ setup.py
```
- [ ] é…ç½®CMakeæ„å»ºç³»ç»Ÿï¼Œæ”¯æŒC++17å’ŒCUDA
- [ ] è®¾ç½®Python Cæ‰©å±•ç¼–è¯‘
- [ ] é…ç½®CI/CDæµæ°´çº¿
- [ ] å»ºç«‹ä»£ç é£æ ¼å’Œè´¨é‡æ£€æŸ¥

#### 1.2 å¼ é‡æ ¸å¿ƒåº“ (ATen)
**å‚è€ƒ**: `pytorch/aten/src/ATen/`

```cpp
// csrc/aten/src/ATen/core/Tensor.h
class TORCH_API Tensor {
public:
  // å‚è€ƒpytorchå®ç°å¼ é‡æ ¸å¿ƒæ¥å£
  Tensor(const Tensor& other);
  Tensor& operator=(const Tensor& other);
  
  // åŸºç¡€å±æ€§
  IntArrayRef sizes() const;
  IntArrayRef strides() const;
  int64_t dim() const;
  ScalarType dtype() const;
  Device device() const;
  
  // æ ¸å¿ƒæ“ä½œ
  Tensor& add_(const Tensor& other);
  Tensor& mul_(const Tensor& other);
  Tensor& matmul_(const Tensor& other);
  
private:
  c10::intrusive_ptr<TensorImpl> impl_;
};
```

#### 1.3 åº•å±‚å¼ é‡å®ç°
**å‚è€ƒ**: `pytorch/c10/core/` å’Œ `pytorch/aten/src/TH/`

- [ ] å®ç° `TensorImpl` ç±» (å¼ é‡æ•°æ®å­˜å‚¨)
- [ ] å®ç° `Storage` ç±» (å†…å­˜ç®¡ç†)
- [ ] å®ç° `Device` å’Œ `ScalarType` æšä¸¾
- [ ] å®ç°åŸºç¡€çš„å†…å­˜åˆ†é…å™¨

### Phase 2: æ ¸å¿ƒç®—å­å®ç° (Week 4-8)

#### 2.1 CPUç®—å­å®ç°
**å‚è€ƒ**: `pytorch/aten/src/ATen/native/`

```cpp
// csrc/aten/src/ATen/native/BinaryOps.cpp
namespace at { namespace native {

Tensor add_cpu(const Tensor& self, const Tensor& other, const Scalar& alpha) {
  // å‚è€ƒpytorchçš„CPUå®ç°
  return at::empty_like(self).add_(other, alpha);
}

Tensor& add_cpu_(Tensor& self, const Tensor& other, const Scalar& alpha) {
  // å°±åœ°æ“ä½œå®ç°
  auto iter = TensorIterator::binary_op(self, self, other);
  add_stub(iter.device_type(), iter, alpha);
  return self;
}

}}
```

- [ ] åŸºç¡€ç®—æœ¯è¿ç®—: add, sub, mul, div
- [ ] çº¿æ€§ä»£æ•°: mm, bmm, addmm
- [ ] æ¿€æ´»å‡½æ•°: relu, sigmoid, tanh, gelu
- [ ] å½’çº¦æ“ä½œ: sum, mean, max, min
- [ ] ç´¢å¼•æ“ä½œ: index_select, gather, scatter

#### 2.2 CUDAç®—å­å®ç°
**å‚è€ƒ**: `pytorch/aten/src/ATen/native/cuda/`

```cuda
// csrc/aten/src/ATen/native/cuda/BinaryOps.cu
template<typename scalar_t>
__global__ void add_kernel(
    TensorIterator iter,
    scalar_t alpha) {
  // å‚è€ƒpytorchçš„CUDA kernelå®ç°
  GPU_LAMBDA(index) {
    scalar_t a = iter.data<scalar_t>()[iter.strides(1) * index];
    scalar_t b = iter.data<scalar_t>()[iter.strides(2) * index];
    iter.data<scalar_t>()[iter.strides(0) * index] = a + alpha * b;
  });
}

Tensor add_cuda(const Tensor& self, const Tensor& other, const Scalar& alpha) {
  auto result = at::empty_like(self);
  auto iter = TensorIterator::binary_op(result, self, other);
  AT_DISPATCH_ALL_TYPES(iter.dtype(), "add_cuda", [&]() {
    gpu_kernel(iter, add_kernel<scalar_t>, alpha.to<scalar_t>());
  });
  return result;
}
```

- [ ] CUDAå†…æ ¸å®ç°åŸºç¡€è¿ç®—
- [ ] å†…å­˜åˆå¹¶è®¿é—®ä¼˜åŒ–
- [ ] å¤šGPUæ”¯æŒ
- [ ] CuBLASå’ŒCuDNNé›†æˆ

#### 2.3 ç®—å­æ³¨å†Œæœºåˆ¶
**å‚è€ƒ**: `pytorch/aten/src/ATen/core/dispatch/`

```cpp
// csrc/aten/src/ATen/ops/add.cpp
TORCH_LIBRARY_IMPL(aten, CPU, m) {
  m.impl("add.Tensor", TORCH_FN(add_cpu));
  m.impl("add_.Tensor", TORCH_FN(add_cpu_));
}

TORCH_LIBRARY_IMPL(aten, CUDA, m) {
  m.impl("add.Tensor", TORCH_FN(add_cuda));
  m.impl("add_.Tensor", TORCH_FN(add_cuda_));
}
```

### Phase 3: è‡ªåŠ¨å¾®åˆ†å¼•æ“ (Week 9-12)

#### 3.1 è®¡ç®—å›¾æ„å»º
**å‚è€ƒ**: `pytorch/torch/csrc/autograd/`

```cpp
// csrc/autograd/variable.cpp
struct TORCH_API AutogradMeta {
  Variable grad_;
  std::shared_ptr<Node> grad_fn_;
  std::weak_ptr<Node> grad_accumulator_;
  VariableVersion version_counter_;
  bool requires_grad_;
  bool retains_grad_;
  bool is_view_;
};
```

- [ ] å®ç° `Variable` ç±» (å·²åˆå¹¶åˆ°Tensor)
- [ ] å®ç° `Node` åŸºç±»å’Œè®¡ç®—å›¾
- [ ] å®ç°æ¢¯åº¦ç´¯ç§¯æœºåˆ¶
- [ ] å®ç°viewå’Œin-placeæ“ä½œå¤„ç†

#### 3.2 åå‘ä¼ æ’­å¼•æ“
**å‚è€ƒ**: `pytorch/torch/csrc/autograd/engine.cpp`

```cpp
// csrc/autograd/engine.cpp
class Engine {
public:
  variable_list execute(
      const edge_list& roots,
      const variable_list& inputs,
      bool keep_graph,
      bool create_graph,
      const edge_list& outputs = {});
      
private:
  void compute_dependencies(FunctionTask task, ReadyQueue& ready);
  void evaluate_function(
      std::shared_ptr<GraphTask>& graph_task,
      Node* func,
      InputBuffer& inputs,
      const std::shared_ptr<ReadyQueue>& cpu_ready_queue);
};
```

#### 3.3 æ¢¯åº¦å‡½æ•°å®ç°
**å‚è€ƒ**: `pytorch/torch/csrc/autograd/functions/`

```cpp
// csrc/autograd/functions/basic_ops.cpp
struct AddBackward : public TraceableFunction {
  using TraceableFunction::TraceableFunction;
  
  variable_list apply(variable_list&& grads) override {
    auto grad_input = grads[0];
    return {grad_input, grad_input * alpha};
  }
  
  Scalar alpha;
};
```

### Phase 4: Pythonç»‘å®šå±‚ (Week 13-15)

#### 4.1 PyBind11é›†æˆ
**å‚è€ƒ**: `pytorch/torch/csrc/Module.cpp`

```cpp
// csrc/api/src/python_bindings.cpp
PYBIND11_MODULE(_C, m) {
  // å¼ é‡ç»‘å®š
  py::class_<Tensor>(m, "Tensor")
    .def("add", &Tensor::add)
    .def("add_", &Tensor::add_)
    .def("backward", &Tensor::backward)
    .def_property_readonly("grad", &Tensor::grad);
    
  // å‡½æ•°ç»‘å®š
  m.def("add", &torch::add);
  m.def("mm", &torch::mm);
}
```

#### 4.2 Pythonå‰ç«¯å®ç°
**å‚è€ƒ**: `pytorch/torch/`

```python
# torch/__init__.py
from torch._C import *  # Cæ‰©å±•å¯¼å…¥
from torch.tensor import Tensor
from torch.autograd import Variable  # å…¼å®¹æ€§

# å¯¼å‡ºä¸»è¦æ¥å£
__all__ = [
    'Tensor', 'tensor', 'add', 'mm', 'nn', 'optim'
]

def tensor(data, dtype=None, device=None, requires_grad=False):
    """åˆ›å»ºå¼ é‡çš„Pythonæ¥å£"""
    return Tensor._make_subclass(data, dtype, device, requires_grad)
```

### Phase 5: ç¥ç»ç½‘ç»œæ¨¡å— (Week 16-20)

#### 5.1 ModuleåŸºç±»
**å‚è€ƒ**: `pytorch/torch/nn/modules/module.py`

```python
# torch/nn/modules/module.py
class Module:
    def __init__(self):
        self._parameters = OrderedDict()
        self._buffers = OrderedDict()
        self._modules = OrderedDict()
        self.training = True
        
    def forward(self, *input):
        raise NotImplementedError
        
    def __call__(self, *input, **kwargs):
        result = self.forward(*input, **kwargs)
        # Hookæœºåˆ¶
        for hook in self._forward_hooks.values():
            hook_result = hook(self, input, result)
            if hook_result is not None:
                result = hook_result
        return result
        
    def register_parameter(self, name, param):
        if '_parameters' not in self.__dict__:
            raise AttributeError("cannot assign parameter before Module.__init__() call")
        self._parameters[name] = param
        
    def parameters(self, recurse=True):
        for name, param in self.named_parameters(recurse=recurse):
            yield param
```

#### 5.2 æ ¸å¿ƒå±‚å®ç°
**å‚è€ƒ**: `pytorch/torch/nn/modules/`

```python
# torch/nn/modules/linear.py
class Linear(Module):
    def __init__(self, in_features, out_features, bias=True):
        super(Linear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.empty((out_features, in_features)))
        if bias:
            self.bias = Parameter(torch.empty(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()
        
    def forward(self, input):
        return F.linear(input, self.weight, self.bias)
        
    def reset_parameters(self):
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            init.uniform_(self.bias, -bound, bound)
```

### Phase 6: ä¼˜åŒ–å™¨å®ç° (Week 21-22)

#### 6.1 ä¼˜åŒ–å™¨åŸºç±»
**å‚è€ƒ**: `pytorch/torch/optim/optimizer.py`

```python
# torch/optim/optimizer.py
class Optimizer:
    def __init__(self, params, defaults):
        self.defaults = defaults
        self.state = defaultdict(dict)
        self.param_groups = []
        
        if isinstance(params, torch.Tensor):
            raise TypeError("params argument given to the optimizer should be "
                          "an iterable of Tensors or dicts, but got " +
                          torch.typename(params))
                          
        self.add_param_group({'params': list(params)})
        
    def step(self, closure=None):
        raise NotImplementedError
        
    def zero_grad(self, set_to_none=False):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    if set_to_none:
                        p.grad = None
                    else:
                        if p.grad.grad_fn is not None:
                            p.grad.detach_()
                        else:
                            p.grad.requires_grad_(False)
                        p.grad.zero_()
```

#### 6.2 å…·ä½“ä¼˜åŒ–å™¨
**å‚è€ƒ**: `pytorch/torch/optim/`

```python
# torch/optim/sgd.py
class SGD(Optimizer):
    def __init__(self, params, lr=1e-3, momentum=0, dampening=0,
                 weight_decay=0, nesterov=False):
        if not 0.0 <= lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if momentum < 0.0:
            raise ValueError("Invalid momentum value: {}".format(momentum))
        if weight_decay < 0.0:
            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))

        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,
                       weight_decay=weight_decay, nesterov=nesterov)
        super(SGD, self).__init__(params, defaults)

    def step(self, closure=None):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            params_with_grad = []
            grads = []
            momentum_buffer_list = []
            
            for p in group['params']:
                if p.grad is not None:
                    params_with_grad.append(p)
                    grads.append(p.grad)
                    
                    state = self.state[p]
                    if len(state) == 0:
                        state['momentum_buffer'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    momentum_buffer_list.append(state['momentum_buffer'])
            
            F.sgd(params_with_grad,
                  grads,
                  momentum_buffer_list,
                  weight_decay=group['weight_decay'],
                  momentum=group['momentum'],
                  lr=group['lr'],
                  dampening=group['dampening'],
                  nesterov=group['nesterov'])
        
        return loss
```

### Phase 7: é«˜çº§åŠŸèƒ½ (Week 23-26)

#### 7.1 JITç¼–è¯‘ (ç®€åŒ–ç‰ˆ)
**å‚è€ƒ**: `pytorch/torch/jit/`

```python
# torch/jit/__init__.py
def script(obj, optimize=None, _frames_up=0, _rcb=None):
    """å°†Pythonå‡½æ•°/æ¨¡å—ç¼–è¯‘ä¸ºTorchScript"""
    if isinstance(obj, torch.nn.Module):
        return torch.jit._script.script_module(obj, _frames_up=_frames_up + 1)
    else:
        return torch.jit._script.script(obj, _frames_up=_frames_up + 1)

def trace(func, example_inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-5, strict=True):
    """é€šè¿‡è·Ÿè¸ªæ‰§è¡Œè·¯å¾„ç”ŸæˆTorchScript"""
    return torch.jit._trace.trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict)
```

#### 7.2 åˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€
**å‚è€ƒ**: `pytorch/torch/distributed/`

```python
# torch/distributed/__init__.py
def init_process_group(backend, init_method=None, timeout=default_pg_timeout, 
                      world_size=-1, rank=-1, store=None, group_name=""):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„"""
    global _default_pg_init_method
    if init_method is None:
        init_method = _default_pg_init_method
    backend = Backend(backend)
    default_pg = _new_process_group_helper(world_size, rank, [], backend, store, timeout=timeout)
    _update_default_pg(default_pg)

class DistributedDataParallel(Module):
    def __init__(self, module, device_ids=None, output_device=None, 
                 dim=0, broadcast_buffers=True, process_group=None, 
                 bucket_cap_mb=25, find_unused_parameters=False, 
                 check_reduction=False, gradient_as_bucket_view=False):
        super(DistributedDataParallel, self).__init__()
        self.module = module
        self.device_ids = device_ids
        self.output_device = output_device
        self.dim = dim
        self.process_group = process_group
        # DDPå…·ä½“å®ç°...
```

## ğŸ› ï¸ æ„å»ºå’Œå®‰è£…

### ç¯å¢ƒè¦æ±‚
```bash
# ç³»ç»Ÿè¦æ±‚
Python >= 3.8
CMake >= 3.18
CUDA >= 11.0 (å¯é€‰)
cuDNN >= 8.0 (å¯é€‰)

# C++ç¼–è¯‘å™¨
GCC >= 9.0 (Linux)
Clang >= 10.0 (macOS)
MSVC >= 2019 (Windows)
```

### æ„å»ºæ­¥éª¤
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/nash635/tiny-torch.git
cd tiny-torch

# åˆ›å»ºæ„å»ºç›®å½•
mkdir build && cd build

# é…ç½®CMake
cmake .. -DCMAKE_BUILD_TYPE=Release \
         -DWITH_CUDA=ON \
         -DPYTHON_EXECUTABLE=$(which python)

# ç¼–è¯‘
make -j$(nproc)

# å®‰è£…PythonåŒ…
cd ..
pip install -e .
```

### éªŒè¯å®‰è£…
```python
import torch

# åŸºç¡€å¼ é‡æ“ä½œ
x = torch.randn(2, 3, requires_grad=True)
y = x * 2 + 1
z = y.sum()
z.backward()
print(x.grad)

# ç¥ç»ç½‘ç»œ
import torch.nn as nn
model = nn.Sequential(
    nn.Linear(10, 5),
    nn.ReLU(),
    nn.Linear(5, 1)
)
output = model(torch.randn(1, 10))
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### ä¸PyTorchå¯¹æ¯”
```python
# benchmarks/compare_with_pytorch.py
import time
import torch as pytorch
import tiny_torch as torch

def benchmark_matmul(size=1000, iterations=100):
    # PyTorch
    x_pt = pytorch.randn(size, size)
    y_pt = pytorch.randn(size, size)
    
    start = time.time()
    for _ in range(iterations):
        z_pt = pytorch.mm(x_pt, y_pt)
    pytorch_time = time.time() - start
    
    # Tiny-Torch
    x_tt = torch.randn(size, size)
    y_tt = torch.randn(size, size)
    
    start = time.time()
    for _ in range(iterations):
        z_tt = torch.mm(x_tt, y_tt)
    tiny_torch_time = time.time() - start
    
    print(f"PyTorch: {pytorch_time:.4f}s")
    print(f"Tiny-Torch: {tiny_torch_time:.4f}s")
    print(f"Relative performance: {pytorch_time/tiny_torch_time:.2f}x")

if __name__ == "__main__":
    benchmark_matmul()
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•å¥—ä»¶
```bash
# Pythonæµ‹è¯•
python -m pytest test/ -v

# C++æµ‹è¯•
cd build && make test

# æ€§èƒ½åŸºå‡†æµ‹è¯•
python benchmarks/compare_with_pytorch.py
```

### æµ‹è¯•è¦†ç›–ç‡
- å¼ é‡æ“ä½œ: 95%+
- è‡ªåŠ¨å¾®åˆ†: 90%+
- ç¥ç»ç½‘ç»œå±‚: 95%+
- ä¼˜åŒ–å™¨: 90%+

## ğŸ“š å­¦ä¹ èµ„æº

### å¿…è¯»ææ–™
1. [PyTorch Internals](http://blog.ezyang.com/2019/05/pytorch-internals/)
2. [PyTorch Autograd Deep Dive](https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/)
3. [ATen: A TENsor library](https://github.com/pytorch/pytorch/tree/main/aten)
4. [PyTorch C++ API](https://pytorch.org/cppdocs/)

### å…³é”®æŠ€æœ¯æ–‡æ¡£
1. **å¼ é‡æ ¸å¿ƒ**: `pytorch/c10/` å’Œ `pytorch/aten/`
2. **è‡ªåŠ¨å¾®åˆ†**: `pytorch/torch/csrc/autograd/`
3. **JITç¼–è¯‘**: `pytorch/torch/csrc/jit/`
4. **åˆ†å¸ƒå¼**: `pytorch/torch/csrc/distributed/`

### å‚è€ƒå®ç°
- [PyTorchæºç ](https://github.com/pytorch/pytorch)
- [PyTorchæ•™ç¨‹](https://pytorch.org/tutorials/)
- [Deep Learning with PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)

## ğŸ“ æ ¸å¿ƒå­¦ä¹ ç›®æ ‡

é€šè¿‡å®ç°Tiny-Torchï¼Œæ‚¨å°†æ·±å…¥æŒæ¡ï¼š

### 1. åº•å±‚ç³»ç»Ÿè®¾è®¡
- **å†…å­˜ç®¡ç†**: å¼ é‡å­˜å‚¨å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†
- **è®¾å¤‡æŠ½è±¡**: CPU/GPUç»Ÿä¸€ç¼–ç¨‹æ¨¡å‹
- **ç±»å‹ç³»ç»Ÿ**: åŠ¨æ€ç±»å‹å’Œé™æ€ç±»å‹çš„ç»“åˆ

### 2. è®¡ç®—å›¾å’Œè‡ªåŠ¨å¾®åˆ†
- **è®¡ç®—å›¾æ„å»º**: åŠ¨æ€å›¾çš„è®¾è®¡å’Œå®ç°
- **åå‘ä¼ æ’­**: æ¢¯åº¦è®¡ç®—çš„å…·ä½“ç®—æ³•
- **å†…å­˜ä¼˜åŒ–**: æ¢¯åº¦ç´¯ç§¯å’Œé‡Šæ”¾ç­–ç•¥

### 3. é«˜æ€§èƒ½è®¡ç®—
- **å‘é‡åŒ–**: SIMDå’Œå¹¶è¡Œè®¡ç®—ä¼˜åŒ–
- **GPUç¼–ç¨‹**: CUDA kernelçš„ç¼–å†™å’Œä¼˜åŒ–
- **å†…å­˜å±‚æ¬¡**: ç¼“å­˜å‹å¥½çš„æ•°æ®è®¿é—®æ¨¡å¼

### 4. ç³»ç»Ÿé›†æˆ
- **Python Cæ‰©å±•**: é«˜æ€§èƒ½Pythonåº“çš„å¼€å‘
- **æ„å»ºç³»ç»Ÿ**: å¤æ‚C++é¡¹ç›®çš„ç»„ç»‡å’Œç¼–è¯‘
- **APIè®¾è®¡**: æ˜“ç”¨æ€§å’Œæ€§èƒ½çš„å¹³è¡¡

## ğŸ¤ è´¡çŒ®æŒ‡å—

### ä»£ç é£æ ¼
- C++: éµå¾ªPyTorchçš„ä»£ç é£æ ¼ (åŸºäºGoogle Style Guide)
- Python: éµå¾ªPEP 8å’ŒPyTorchçº¦å®š
- æ³¨é‡Š: è¯¦ç»†çš„å®ç°æ³¨é‡Šå’ŒAPIæ–‡æ¡£

### æäº¤æµç¨‹
1. Forkæœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. ç¡®ä¿æµ‹è¯•é€šè¿‡ (`python -m pytest test/`)
4. æäº¤ä»£ç  (`git commit -m 'Add amazing feature'`)
5. æ¨é€åˆ†æ”¯ (`git push origin feature/amazing-feature`)
6. åˆ›å»ºPull Request

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# å®‰è£…pre-commit hooks
pre-commit install

# æ ¼å¼åŒ–ä»£ç 
black torch/ test/
clang-format -i csrc/**/*.cpp csrc/**/*.h
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨BSD 3-Clauseè®¸å¯è¯ï¼Œä¸PyTorchä¿æŒä¸€è‡´ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- **PyTorchå›¢é˜Ÿ**: æä¾›äº†å“è¶Šçš„æ·±åº¦å­¦ä¹ æ¡†æ¶è®¾è®¡
- **Facebook AI Research**: å¼€æºäº†PyTorchçš„æ ¸å¿ƒå®ç°
- **NVIDIA**: æä¾›äº†CUDAå’ŒcuDNNæ”¯æŒ
- **å¼€æºç¤¾åŒº**: ä¸ºæ·±åº¦å­¦ä¹ æ¡†æ¶å‘å±•åšå‡ºçš„è´¡çŒ®

---

**å¼€å§‹æ‚¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶æ¢ç´¢ä¹‹æ—…ï¼** ğŸš€

é€šè¿‡å®ç°Tiny-Torchï¼Œæ‚¨å°†è·å¾—å¯¹ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶åº•å±‚æœºåˆ¶çš„æ·±åˆ»ç†è§£ï¼Œè¿™å¯¹äºæ·±åº¦å­¦ä¹ ç ”ç©¶å’Œå·¥ç¨‹å®è·µéƒ½å…·æœ‰é‡è¦ä»·å€¼ã€‚

å¦‚æœåœ¨å®ç°è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œæ¬¢è¿æäº¤ [Issues](../../issues) æˆ–å‚ä¸ [Discussions](../../discussions)ã€‚
